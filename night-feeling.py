import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as stats
import matplotlib.pyplot as plt

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)



#ìì¹˜êµ¬, ë²”ì£„ìœ¨, ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´,ì™¸), ì´ ë²”ì£„ê±´ìˆ˜, êµ¬ë³„ ê²½ì°°ìˆ˜
crime_rate_data = pd.read_csv('./data/crime_rate.csv', encoding='cp949')

#ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ 1ì¸ê°€êµ¬ìˆ˜ ì •ë³´
one_housed = pd.read_excel('./data/seoul_one_person_housed_updated.xlsx')
#ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ íŒŒì¶œì†Œ ìˆ˜
SeoulSafetyCenter = pd.read_excel('./data/Seoul_SafetyCener_info.xlsx')
#ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ ì•ˆì „ë²¨ ìˆ˜
bell = pd.read_excel('./data/Seoul_Safetybell.xlsx', engine='openpyxl')
#ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ ìœ í¥ì—…ì†Œ ìˆ˜
shop = pd.read_csv('./data/ë¨¸ì§€í•œìœ í¥ì—…ì†Œë°ì´í„°.csv', encoding='utf-8')



import pandas as pd
raw_df = crime_rate_data.copy()
#  ë¬¸ìì—´ ë¶„ë¦¬ (\t split)
split_data = raw_df.iloc[:, 0].str.split('\t', expand=True)
#  ì»¬ëŸ¼ëª…
split_data.columns = ['ìì¹˜êµ¬', 'ì´ë²”ì£„ê±´ìˆ˜', 'ìì¹˜êµ¬ì½”ë“œ', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ì™¸)', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ë²”ì£„ìœ¨', 'êµ¬ë³„ ê²½ì°°ìˆ˜','ë¹ˆì¹¸']

#  ìˆ«ìí˜• ì»¬ëŸ¼ floatë³€í™˜
cols_to_float = ['ì´ë²”ì£„ê±´ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ì™¸)', 'ì´ìƒí™œì¸êµ¬ìˆ˜', 'ë²”ì£„ìœ¨', 'êµ¬ë³„ ê²½ì°°ìˆ˜']
for col in cols_to_float:
    split_data[col] = pd.to_numeric(split_data[col], errors='coerce')

split_data = split_data.drop(columns=['ë¹ˆì¹¸'])

split_data.head()

import pandas as pd
import statsmodels.api as sm


#  1ì¸ê°€êµ¬ ë°ì´í„° ì „ì²˜ë¦¬
one_housed_clean = one_housed.rename(columns={'ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ìˆ˜': 'ìì¹˜êµ¬', 'ê³„': '1ì¸ê°€êµ¬ìˆ˜'})
one_housed_clean = one_housed_clean[['ìì¹˜êµ¬', '1ì¸ê°€êµ¬ìˆ˜']]

#  íŒŒì¶œì†Œ ê°œìˆ˜ ì„¸ê¸°
station_counts = SeoulSafetyCenter['ìì¹˜êµ¬'].value_counts().reset_index()
station_counts.columns = ['ìì¹˜êµ¬', 'íŒŒì¶œì†Œìˆ˜']


# cctvì´ ìˆ˜ëŸ‰
cctv = pd.read_csv('./data/Seoul_CCTV_info.csv', encoding='cp949')
cctv_by_gu = cctv.groupby('ìì¹˜êµ¬')['CCTV ìˆ˜ëŸ‰'].sum().reset_index()
cctv_by_gu.columns = ['ìì¹˜êµ¬', 'CCTV ì´ìˆ˜ëŸ‰']
print(cctv_by_gu)

#ì„¸ì´í”„í‹° ë°¸ ìˆ˜ëŸ‰
bell = bell.groupby('ìì¹˜êµ¬')['ë²ˆí˜¸'].count()

#ìœ í¥ì—…ì†Œ ìˆ˜ëŸ‰
shop=shop.groupby('ìì¹˜êµ¬')['ì´_ê°œìˆ˜'].sum()



#  ê¸°ì¡´ ë²”ì£„ìœ¨ ë°ì´í„° 

#  ë³‘í•©
merged_df = split_data.merge(one_housed_clean, on='ìì¹˜êµ¬', how='left')
merged_df = merged_df.merge(station_counts, on='ìì¹˜êµ¬', how='left')
merged_df = merged_df.merge(cctv_by_gu, on='ìì¹˜êµ¬', how='left')
merged_df = merged_df.merge(bell, on='ìì¹˜êµ¬', how='left')
merged_df = merged_df.merge(shop, on='ìì¹˜êµ¬', how='left')



# ê²°ì¸¡ì¹˜ í™•ì¸ í›„ ì²˜ë¦¬ (ì˜ˆ: ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ëŒ€ì²´)
merged_df.fillna(0, inplace=True)


# ë…ë¦½ ë³€ìˆ˜(X)ì™€ ì¢…ì† ë³€ìˆ˜(y) ì§€ì •
X = merged_df[['ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ì™¸)', 'ì´ë²”ì£„ê±´ìˆ˜', '1ì¸ê°€êµ¬ìˆ˜', 'íŒŒì¶œì†Œìˆ˜','CCTV ì´ìˆ˜ëŸ‰','ë²ˆí˜¸','ì´_ê°œìˆ˜']]
y = merged_df['ë²”ì£„ìœ¨']

X = sm.add_constant(X)
model = sm.OLS(y, X).fit()

print(model.summary())


import statsmodels.api as sm

# Xì— ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ë§Œ ì„ íƒ
X_sig = merged_df['CCTV ì´ìˆ˜ëŸ‰']
y = merged_df['ë²”ì£„ìœ¨']
X_sig = sm.add_constant(X_sig)
model_sig = sm.OLS(y, X_sig).fit()
# ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print(model_sig.summary())


merged_df.columns
merged_df.head()
merged_df.info()


# ì”ì°¨ì™€ ì˜ˆì¸¡ê°’ êµ¬í•˜ê¸°
fitted_vals = model.fittedvalues
residuals = model.resid

#  ì •ê·œì„± ê²€ì •
# ì”ì°¨(ì˜¤ì°¨)ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸
# ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¼ì•¼ íšŒê·€ê³„ìˆ˜ì˜
# ì‹ ë¢°êµ¬ê°„, t-ê²€ì •, F-ê²€ì • ë“±ì˜ í†µê³„ì  ì¶”ë¡ ì´ ì •í™•

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import statsmodels.api as sm

# Q-Q plot
# ì”ì°¨ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸
sm.qqplot(residuals, line='45', fit=True)
plt.title('Q-Q Plot of Residuals')
plt.show()

# íˆìŠ¤í† ê·¸ë¨
# ì”ì°¨ ë¶„í¬ì˜ ì „ì²´ì ì¸ ëª¨ì–‘ í™•ì¸
sns.histplot(residuals, kde=True)
plt.title('Histogram of Residuals')
plt.xlabel('Residuals')
plt.show()

# Shapiro-Wilk Test
# Shapiro-Wilk Test: ì •ê·œì„±ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê²€ì •

from scipy.stats import shapiro
stat, p = shapiro(residuals)
print(f'Shapiro-Wilk Test: stat={stat:.4f}, p-value={p:.4f}')
# p > 0.05ë©´ ì •ê·œì„± ê°€ì • ë§Œì¡±


# ë“±ë¶„ì‚°ì„± ê²€ì • (Homoscedasticity)
# ì”ì°¨ì˜ ë¶„ì‚°ì´ ì˜ˆì¸¡ê°’ê³¼ ë¬´ê´€í•˜ê²Œ ì¼ì •í•œì§€ í™•ì¸
# Fitted vs Residuals Plot

plt.scatter(fitted_vals, residuals)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Fitted Values')
plt.xlabel('Fitted values')
plt.ylabel('Residuals')
plt.show()


# Breusch-Pagan Test
# í†µê³„ì ìœ¼ë¡œ ë“±ë¶„ì‚° ì—¬ë¶€ ê²€ì • (p < 0.05ë©´ ì´ë¶„ì‚°ì„± ìˆìŒ)
from statsmodels.stats.diagnostic import het_breuschpagan
bp_test = het_breuschpagan(residuals, X)
bp_labels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']

print(dict(zip(bp_labels, bp_test)))
# p-value > 0.05 â†’ ë“±ë¶„ì‚°ì„± ë§Œì¡±

# ë‹¤ì¤‘ê³µì„ ì„± í™•ì¸ (VIF)
# ëª©ì : ë…ë¦½ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ë„ˆë¬´ ë†’ì€ì§€ í™•ì¸
# ë…ë¦½ ë³€ìˆ˜ë¼ë¦¬ ê°•í•œ ìƒê´€ì´ ìˆìœ¼ë©´ íšŒê·€ê³„ìˆ˜ê°€ ë¶ˆì•ˆì •í•´ì§€ê³  í•´ì„ì´ ì–´ë ¤ì›Œì§
# ì¼ë°˜ì ìœ¼ë¡œ VIF > 10 ì´ë©´ ë‹¤ì¤‘ê³µì„ ì„± ì˜ì‹¬
from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd

vif_df = pd.DataFrame()
vif_df["feature"] = X.columns
vif_df["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_df)
# VIF < 10 â†’ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì—†ìŒ


#  Cook's Distance (ì´ìƒì¹˜ ì˜í–¥ í™•ì¸)
# ëª©ì : ì¼ë¶€ ë°ì´í„°ê°€ íšŒê·€ ê²°ê³¼ì— ì§€ë‚˜ì¹˜ê²Œ í° ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸
# ì˜í–¥ë ¥ ë†’ì€ ì´ìƒì¹˜ëŠ” ì „ì²´ ëª¨ë¸ì„ ì™œê³¡ì‹œí‚¬ ìˆ˜ ìˆìŒ
# ì¼ë°˜ì ìœ¼ë¡œ 0.5 ì´ìƒì´ë©´ ì£¼ì˜, 1 ì´ìƒì´ë©´ ì˜í–¥ë ¥ í° ì´ìƒì¹˜ë¡œ ê°„ì£¼
from statsmodels.stats.outliers_influence import OLSInfluence
influence = OLSInfluence(model)
cooks_d = influence.cooks_distance[0]


# ì‹œê°í™”
plt.figure(figsize=(10, 4))
plt.stem(cooks_d, markerfmt='ro', linefmt='b-', basefmt='k-')
plt.title("Cook's Distance")
plt.xlabel('Observation Index')
plt.ylabel("Cook's Distance")
plt.show()
#0.5~1 ì´ìƒì´ë©´ ì˜í–¥ë ¥ í° ê´€ì¸¡ì¹˜ (ì£¼ì˜ í•„ìš”)



print(shop.head(2))


# ì‹œê°í™” ì‹œì‘

import plotly.express as px
import json
with open('./data/seoul_districts.geojson', encoding='utf-8') as f:
    geojson_data = json.load(f)




fig = px.choropleth_mapbox(
    merged_df,
    geojson=geojson_data,
    locations='ìì¹˜êµ¬',
    featureidkey='properties.SIG_KOR_NM',
    color='ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)',                               
    color_continuous_scale='Reds',                # ë²”ì£„ìœ¨ì€ ì—°ì†ê°’ì´ë¼ ì—°ì† ìƒ‰ìƒ ì‚¬ìš©
    hover_name='ìì¹˜êµ¬',
    hover_data={'ë²”ì£„ìœ¨': True, 'ì´ë²”ì£„ê±´ìˆ˜': True},
    mapbox_style='carto-positron',
    center={'lat': 37.5665, 'lon': 126.9780},
    zoom=10,
    opacity=0.7,
    title='ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ ë²”ì£„ìœ¨ ì‹œê°í™”'
)
fig.show()

merged_df['ì¸êµ¬_ë²”ì£¼'] = pd.qcut(merged_df['ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)'], q=3, labels=['ë‚®ìŒ', 'ì¤‘ê°„', 'ë†’ìŒ'])





#  ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ ëª©ë¡
custom_colors = ['#636EFA', '#EF553B', '#00CC96']  # íŒŒë€ìƒ‰, ë¹¨ê°„ìƒ‰, ì´ˆë¡ìƒ‰

#  Choropleth Mapbox ì‹œê°í™”
fig = px.choropleth_mapbox(
    merged_df,
    geojson=geojson_data,
    locations='ìì¹˜êµ¬',
    featureidkey='properties.SIG_KOR_NM',
    color='ì¸êµ¬_ë²”ì£¼',
    color_discrete_sequence=custom_colors,
    hover_name='ìì¹˜êµ¬',
    hover_data={'ë²”ì£„ìœ¨': True, 'ì´ë²”ì£„ê±´ìˆ˜': True, 'ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)': True},
    mapbox_style='carto-positron',
    center={'lat': 37.5665, 'lon': 126.9780},
    zoom=10,
    opacity=0.7,
    title='ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´) ë²”ì£¼ ì‹œê°í™”'
)

fig.show()




import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.express as px


# í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  ë³€ìˆ˜ ì„ íƒ
features = ['ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)', 'ì´ìƒí™œì¸êµ¬ìˆ˜(ì™¸)', '1ì¸ê°€êµ¬ìˆ˜', 'íŒŒì¶œì†Œìˆ˜', 'CCTV ì´ìˆ˜ëŸ‰']

# ì„ íƒí•œ ë³€ìˆ˜ë¡œë¶€í„° ë°ì´í„° ì¶”ì¶œ
X = merged_df[features]

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# êµ°ì§‘ ê°œìˆ˜ ì„¤ì •
k_values = [2, 3]

for k in k_values:
    # KMeans ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    kmeans = KMeans(n_clusters=k, random_state=42)
    merged_df[f'í´ëŸ¬ìŠ¤í„°_{k}'] = kmeans.fit_predict(X_scaled)


def plot_cluster_map(df, cluster_col, title):
    # êµ°ì§‘ ê°œìˆ˜ì— ë”°ë¼ ìƒ‰ìƒ ëª©ë¡ ì •ì˜
    num_clusters = df[cluster_col].nunique()
    color_map = {
        2: ['#636EFA', '#EF553B'],
        3: ['#636EFA', '#EF553B', '#00CC96']
    }
    colors = color_map.get(num_clusters, px.colors.qualitative.Plotly)

    fig = px.choropleth_mapbox(
        df,
        geojson=geojson_data,
        locations='ìì¹˜êµ¬',
        featureidkey='properties.SIG_KOR_NM',
        color=cluster_col,
        color_discrete_sequence=colors,
        hover_name='ìì¹˜êµ¬',
        hover_data={
            'ë²”ì£„ìœ¨': True,
            'ì´ë²”ì£„ê±´ìˆ˜': True,
            'ì´ìƒí™œì¸êµ¬ìˆ˜(ë‚´)': True,
            '1ì¸ê°€êµ¬ìˆ˜': True,
            'íŒŒì¶œì†Œìˆ˜': True,
            'CCTV ì´ìˆ˜ëŸ‰': True
        },
        mapbox_style='carto-positron',
        center={'lat': 37.5665, 'lon': 126.9780},
        zoom=10,
        opacity=0.7,
        title=title
    )
    fig.show()


# êµ°ì§‘ ê°œìˆ˜ 2ê°œ ì‹œê°í™”
plot_cluster_map(merged_df, 'í´ëŸ¬ìŠ¤í„°_2', 'ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ (k=2)')

# êµ°ì§‘ ê°œìˆ˜ 3ê°œ ì‹œê°í™”
plot_cluster_map(merged_df, 'í´ëŸ¬ìŠ¤í„°_3', 'ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ (k=3)')





############################################################
#í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶€ë¥´ê¸°
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from sklearn.metrics import r2_score
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
from sklearn.preprocessing import StandardScaler
from scipy import stats
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.stattools import durbin_watson
plt.rcParams['font.family'] ='Malgun Gothic'
plt.rcParams['axes.unicode_minus'] =False




food_and_entertain = pd.read_csv('./data/ë¨¸ì§€í•œìœ í¥ì—…ì†Œë°ì´í„°.csv', encoding='utf-8')
seoul_safetybell = pd.read_excel('./data/Seoul_Safetybell.xlsx', engine='openpyxl')
cctv = pd.read_csv('./data/Seoul_CCTV_info.csv',encoding='cp949')
crime_rate = pd.read_csv('./data/crime_rate.csv',encoding='euc-kr',sep='\t')
one_housed = pd.read_excel('./data/seoul_one_person_housed_updated.xlsx')
SeoulSafetyCenter = pd.read_excel('./data/Seoul_SafetyCener_info.xlsx')

#ì•ˆì „ë²¨ ê°œìˆ˜
seoul_safetybell_df = seoul_safetybell.groupby('ìì¹˜êµ¬')['ë²ˆí˜¸'].count()
#cctv ê°œìˆ˜
cctv_df = cctv.groupby('ìì¹˜êµ¬')['CCTV ìˆ˜ëŸ‰'].sum()

#  1ì¸ê°€êµ¬ ìˆ˜
one_housed_clean = one_housed.rename(columns={'ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ìˆ˜': 'ìì¹˜êµ¬', 'ê³„': '1ì¸ê°€êµ¬ìˆ˜'})
one_housed_clean = one_housed_clean[['ìì¹˜êµ¬', '1ì¸ê°€êµ¬ìˆ˜']]

#  íŒŒì¶œì†Œ ê°œìˆ˜
station_counts = SeoulSafetyCenter['ìì¹˜êµ¬'].value_counts().reset_index()
station_counts.columns = ['ìì¹˜êµ¬', 'íŒŒì¶œì†Œìˆ˜']


df = crime_rate.merge(seoul_safetybell_df, on='ìì¹˜êµ¬') \
                       .merge(cctv_df, on='ìì¹˜êµ¬') \
                       .merge(food_and_entertain, on='ìì¹˜êµ¬') \
                       .merge(one_housed_clean, on='ìì¹˜êµ¬') \
                       .merge(station_counts, on='ìì¹˜êµ¬')
df = df.rename(columns={'ë²ˆí˜¸': 'ì•ˆì „ë²¨ ê°œìˆ˜','ì´_ê°œìˆ˜': 'ì´ ìŒì‹ì  ìˆ˜'})
df = df.drop(columns=['Unnamed: 8','ì¼ë°˜ìŒì‹ì _ê°œìˆ˜','ìœ í¥ì—…ì†Œ_ê°œìˆ˜','ìì¹˜êµ¬ì½”ë“œ'])


########################################################################

df2 = df.copy()


scaler = StandardScaler()
x = ['ì´ìƒí™œì¸êµ¬ìˆ˜','ì´ë²”ì£„ê±´ìˆ˜','êµ¬ë³„ ê²½ì°°ìˆ˜','ì•ˆì „ë²¨ ê°œìˆ˜','CCTV ìˆ˜ëŸ‰','ì´ ìŒì‹ì  ìˆ˜','1ì¸ê°€êµ¬ìˆ˜','íŒŒì¶œì†Œìˆ˜']
X_scaled = scaler.fit_transform(df2[x])


df2 = pd.DataFrame(X_scaled, columns=x, index=df2.index)


#ìš°ë¦¬ê°€ ìƒê°í•˜ê¸°ì— ë²”ì£„ìœ¨ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ìƒê°í•˜ëŠ” ë³€ìˆ˜ë“¤
X3 = df2[['ì´ ìŒì‹ì  ìˆ˜','CCTV ìˆ˜ëŸ‰','1ì¸ê°€êµ¬ìˆ˜','êµ¬ë³„ ê²½ì°°ìˆ˜','ì´ìƒí™œì¸êµ¬ìˆ˜']]
y = df2['ì´ë²”ì£„ê±´ìˆ˜']
X3 = sm.add_constant(X3)
model3 = sm.OLS(y, X3).fit()
print(model3.summary())


#aicë¡œ stepwiseí•œê±°
X1 = df2[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'êµ¬ë³„ ê²½ì°°ìˆ˜','ì•ˆì „ë²¨ ê°œìˆ˜' ,'CCTV ìˆ˜ëŸ‰', 'ì´ ìŒì‹ì  ìˆ˜', '1ì¸ê°€êµ¬ìˆ˜','íŒŒì¶œì†Œìˆ˜']]
y = df2['ì´ë²”ì£„ê±´ìˆ˜']

lr = LinearRegression()
names = X1.columns
def aic_score(estimator,X1, y):
    X1 = sm.add_constant(X1) 
    model = sm.OLS(y, X1).fit()
    print("Model AIC:", model.aic)
    return -model.aic
# Perform SFS
sfs = SFS(lr,
          k_features=(1,7),   
          forward=True,      
          scoring=aic_score,  
          cv=0,
          verbose = 0)
sfs.fit(X1, y)

print('Selected features:', np.array(names)[list(sfs.k_feature_idx_)])


x1 = df2[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'êµ¬ë³„ ê²½ì°°ìˆ˜', 'CCTV ìˆ˜ëŸ‰', 'ì´ ìŒì‹ì  ìˆ˜', 'íŒŒì¶œì†Œìˆ˜']]
x1 = sm.add_constant(x1)
model1 = sm.OLS(y, x1).fit()
print(model1.summary())



#adj-r2 ê¸°ì¤€ ë³€ìˆ˜ ì„ íƒ
X2 = df2[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'êµ¬ë³„ ê²½ì°°ìˆ˜','ì•ˆì „ë²¨ ê°œìˆ˜' ,'CCTV ìˆ˜ëŸ‰', 'ì´ ìŒì‹ì  ìˆ˜', '1ì¸ê°€êµ¬ìˆ˜','íŒŒì¶œì†Œìˆ˜']]
y = df2['ì´ë²”ì£„ê±´ìˆ˜']
# Adj R2 ìŠ¤ì½”ì–´ í•¨ìˆ˜ ì •ì˜
def adjusted_r2_score(estimator, X2, y):
    y_pred = estimator.predict(X2)
    n = X2.shape[0]
    p = X2.shape[1]
    r2 = r2_score(y, y_pred)
    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    return adjusted_r2


sfs = SFS(lr,
          k_features=(1,7),
          forward=True,
          scoring=adjusted_r2_score,
          cv=0,
          verbose = 2)

sfs.fit(X2, y)


selected_indices_r2 = list(sfs.k_feature_idx_)
names_r2 = np.array(X2.columns)[:-1]

x2 = df2[['ì´ìƒí™œì¸êµ¬ìˆ˜', 'êµ¬ë³„ ê²½ì°°ìˆ˜', 'ì•ˆì „ë²¨ ê°œìˆ˜', 'CCTV ìˆ˜ëŸ‰', 'ì´ ìŒì‹ì  ìˆ˜', '1ì¸ê°€êµ¬ìˆ˜']]
x2 = sm.add_constant(x2)
model2 = sm.OLS(y, x2).fit()
print(model2.summary())



model1.aic
model2.aic
model3.aic

#ìµœì¢… ì„ ì • ëª¨ë¸
print(model1.summary())

# ì”ì°¨ì •ê·œì„±
residuals = model1.resid
fitted_values = model1.fittedvalues
plt.figure(figsize=(15,4))
plt.subplot(1,2,1)
plt.scatter(fitted_values, residuals)
plt.axhline(y=0, color='black', linestyle='--')
plt.subplot(1,2,2)
stats.probplot(residuals, plot=plt)
plt.show()

resid_stats, resid_pvalue = stats.shapiro(residuals)
print(resid_pvalue)
#p_valueê°€ 0.07ë¡œ ìœ ì˜ìˆ˜ì¤€ 5%í•˜ì—ì„œ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ì—†ë‹¤
#ì”ì°¨ëŠ” ì •ê·œì„±ì„ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.



#ì”ì°¨ ë“±ë¶„ì‚°ì„±
bptest = het_breuschpagan(model1.resid, model1.model.exog)
print('BP-test statistics: ', bptest[0])
print('BP-test p_value: ', bptest[1]) #p-valueê°€ 0.3ìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì§€ ëª»í•¨
#ë“±ë¶„ì‚°ì„± ë§Œì¡±í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ


#ì”ì°¨ ë…ë¦½ì„±
dw_stat = durbin_watson(model1.resid)
print(dw_stat)  
#2 ì •ë„ë¡œ ì”ì°¨ ë…ë¦½ì„± ë§Œì¡±í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ


# -----------------------------------------------------

# í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  ë³€ìˆ˜ ì„ íƒ
features = ['êµ¬ë³„ ê²½ì°°ìˆ˜', 'ì´ ìŒì‹ì  ìˆ˜', 'ì´ìƒí™œì¸êµ¬ìˆ˜',  'íŒŒì¶œì†Œìˆ˜']

# ì„ íƒí•œ ë³€ìˆ˜ë¡œë¶€í„° ë°ì´í„° ì¶”ì¶œ
X = df2[features]

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# êµ°ì§‘ ê°œìˆ˜ ì„¤ì •
k_values = [2]

for k in k_values:
    # KMeans ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    kmeans = KMeans(n_clusters=k, random_state=42)
    df2[f'í´ëŸ¬ìŠ¤í„°_{k}'] = kmeans.fit_predict(X_scaled)






def plot_cluster_map(df, cluster_col, title):
    # ë³µì‚¬ë³¸ ìƒì„± ë° êµ°ì§‘ ë²ˆí˜¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ ë³€ê²½
    df_viz = df.copy()
    unique_clusters = sorted(df_viz[cluster_col].unique())
    
    # êµ°ì§‘ ë²ˆí˜¸ë¥¼ "êµ°ì§‘ 1", "êµ°ì§‘ 2" í˜•íƒœë¡œ ë§¤í•‘
    cluster_name_map = {cid: f"êµ°ì§‘ {i+1}" for i, cid in enumerate(unique_clusters)}
    df_viz['êµ°ì§‘ëª…'] = df_viz[cluster_col].map(cluster_name_map)

    # ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ìƒ ì¡°í•© (ex: ì—°í•œ íŒŒë‘/ì—°í•œ ì£¼í™©/ì—°í•œ ì´ˆë¡)
    natural_colors = {
        2: ['#A6CEE3', '#FDBF6F'],      # ì—°íŒŒë‘, ì—°ì£¼í™©
        3: ['#A6CEE3', '#FDBF6F', '#B2DF8A']  # + ì—°ë…¹ìƒ‰
    }
    colors = natural_colors.get(len(unique_clusters), px.colors.qualitative.Set2)

    fig = px.choropleth_mapbox(
        df_viz,
        geojson=geojson_data,
        locations='ìì¹˜êµ¬',
        featureidkey='properties.SIG_KOR_NM',
        color='êµ°ì§‘ëª…',  # ë²”ë¡€ì™€ hoverì— "êµ°ì§‘ n" ìœ¼ë¡œ í‘œì‹œ
        color_discrete_sequence=colors,
        hover_name='ìì¹˜êµ¬',
        hover_data={
            'êµ°ì§‘ëª…': True,
            'êµ¬ë³„ ê²½ì°°ìˆ˜': True,
            'ì´ ìŒì‹ì  ìˆ˜': True,
            'ì´ìƒí™œì¸êµ¬ìˆ˜': True,
            'íŒŒì¶œì†Œìˆ˜': True,
        },
        mapbox_style='carto-positron',
        center={'lat': 37.5665, 'lon': 126.9780},
        zoom=10,
        opacity=0.7,
        title=title
    )
    
    # ìƒ‰ìƒ ë°” ì œê±° (ì—°ì†í˜• ì•„ë‹˜)
    fig.update_layout(coloraxis_showscale=False)
    fig.show()

# êµ°ì§‘ ê°œìˆ˜ 2ê°œ ì‹œê°í™”
plot_cluster_map(df2, 'í´ëŸ¬ìŠ¤í„°_2', 'ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§ (k=2)')

# ---------------------------------------

def show_top_correlations_by_cluster(df, cluster_col, features, top_n=5, top_gu_n=3, gu_sort_key='ì´ìƒí™œì¸êµ¬ìˆ˜'):
    for idx, cluster_label in enumerate(sorted(df[cluster_col].unique()), start=1):
        print(f"\nğŸ”¹ êµ°ì§‘ {idx} ìƒìœ„ {top_n} ìƒê´€ ë³€ìˆ˜ìŒ")

        # í•´ë‹¹ êµ°ì§‘ ë°ì´í„°ë§Œ ì¶”ì¶œ
        cluster_df = df[df[cluster_col] == cluster_label]
        cluster_data = cluster_df[features]

        # ìƒê´€ê´€ê³„ ê³„ì‚°
        corr_matrix = cluster_data.corr().abs()

        # ìê¸° ìì‹ ê³¼ì˜ ìƒê´€ ì œê±° í›„, ìƒìœ„ top_n ì¶”ì¶œ
        corr_pairs = (
            corr_matrix.where(~np.eye(corr_matrix.shape[0], dtype=bool))
            .stack()
            .sort_values(ascending=False)
            .drop_duplicates()
        )

        top_corrs = corr_pairs.head(top_n)

        for (var1, var2), corr_val in top_corrs.items():
            print(f"  ğŸ“Œ {var1} & {var2} â†’ ìƒê´€ê³„ìˆ˜: {corr_val:.2f}")

        # ëŒ€í‘œ ìì¹˜êµ¬ ì¶œë ¥ (ê¸°ì¤€ ë³€ìˆ˜ë¡œ ì •ë ¬)
        if gu_sort_key in cluster_df.columns:
            sorted_cluster_df = cluster_df.sort_values(by=gu_sort_key, ascending=False)
            gu_list = sorted_cluster_df['ìì¹˜êµ¬'].head(top_gu_n).tolist()
        else:
            gu_list = cluster_df['ìì¹˜êµ¬'].head(top_gu_n).tolist()  # ëŒ€ì²´: ì •ë ¬ ê¸°ì¤€ ì—†ì„ ê²½ìš°

        print(f"\n    ğŸ™ï¸ ëŒ€í‘œ ìì¹˜êµ¬ (ê¸°ì¤€: {gu_sort_key}, ìƒìœ„ {top_gu_n}ê°œ): {', '.join(gu_list)}")



show_top_correlations_by_cluster(df2, 'í´ëŸ¬ìŠ¤í„°_2', features, top_n=5, top_gu_n=3)


# êµ°ì§‘ 1ì€ ì´ìƒí™œì¸êµ¬ìˆ˜ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ê°•í•œ ì–‘ì˜ ìƒê´€ì„ ê°€ì§€ë©°, 
# ì¸êµ¬ê°€ ë§ì€ ì§€ì—­ì¼ìˆ˜ë¡ ìŒì‹ì ë„ ë§ê³ , ê²½ì°° ìˆ˜ì™€ íŒŒì¶œì†Œ ìˆ˜ë„ ë§ìŒ â†’ ìƒì—…Â·ìœ ë™ ì¸êµ¬ ì¤‘ì‹¬í˜• ì§€ì—­

# êµ°ì§‘ 2ëŠ” ì´ìƒí™œì¸êµ¬ìˆ˜ì™€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ì´ ë‚®ê³ , 
# ëŒ€ì‹  êµ¬ë³„ ê²½ì°°ìˆ˜ì™€ íŒŒì¶œì†Œìˆ˜ ê°„ì—ë§Œ ë†’ì€ ìƒê´€ â†’ ì£¼ê±°í˜•/ê³µê³µì•ˆì „ ì¤‘ì‹¬í˜• ì§€ì—­ì¼ ê°€ëŠ¥ì„±

